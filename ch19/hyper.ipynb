{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from scipy import stats\n",
    "from torch import nn\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOTrainer(d2l.Trainer):  #@save\n",
    "    def validation_error(self):\n",
    "        self.model.eval()\n",
    "        accuracy = 0\n",
    "        val_batch_idx = 0\n",
    "        for batch in self.val_dataloader:\n",
    "            with torch.no_grad():\n",
    "                x, y = self.prepare_batch(batch)\n",
    "                y_hat = self.model(x)\n",
    "                accuracy += self.model.accuracy(y_hat, y)\n",
    "            val_batch_idx += 1\n",
    "        return 1 -  accuracy / val_batch_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpo_objective_softmax_classification(config, max_epochs=8):\n",
    "    learning_rate = config[\"learning_rate\"]\n",
    "    trainer = d2l.HPOTrainer(max_epochs=max_epochs)\n",
    "    data = d2l.FashionMNIST(batch_size=16)\n",
    "    model = d2l.SoftmaxRegression(num_outputs=10, lr=learning_rate)\n",
    "    trainer.fit(model=model, data=data)\n",
    "    return trainer.validation_error().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space = {\"learning_rate\": stats.loguniform(1e-4, 1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors, values = [], []\n",
    "num_iterations = 5\n",
    "\n",
    "for i in range(num_iterations):\n",
    "    learning_rate = config_space[\"learning_rate\"].rvs()\n",
    "    print(f\"Trial {i}: learning_rate = {learning_rate}\")\n",
    "    y = hpo_objective_softmax_classification({\"learning_rate\": learning_rate})\n",
    "    print(f\"    validation_error = {y}\")\n",
    "    values.append(learning_rate)\n",
    "    errors.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = np.argmin(errors)\n",
    "print(f\"optimal learning rate = {values[best_idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from scipy import stats\n",
    "from d2l import torch as d2l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOSearcher(d2l.HyperParameters):  #@save\n",
    "    def sample_configuration() -> dict:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, config: dict, error: float, additional_info=None):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomSearcher(HPOSearcher):  #@save\n",
    "    def __init__(self, config_space: dict, initial_config=None):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def sample_configuration(self) -> dict:\n",
    "        if self.initial_config is not None:\n",
    "            result = self.initial_config\n",
    "            self.initial_config = None\n",
    "        else:\n",
    "            result = {\n",
    "                name: domain.rvs()\n",
    "                for name, domain in self.config_space.items()\n",
    "            }\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOScheduler(d2l.HyperParameters):  #@save\n",
    "    def suggest(self) -> dict:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def update(self, config: dict, error: float, info=None):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicScheduler(HPOScheduler):  #@save\n",
    "    def __init__(self, searcher: HPOSearcher):\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def suggest(self) -> dict:\n",
    "        return self.searcher.sample_configuration()\n",
    "\n",
    "    def update(self, config: dict, error: float, info=None):\n",
    "        self.searcher.update(config, error, additional_info=info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPOTuner(d2l.HyperParameters):  #@save\n",
    "    def __init__(self, scheduler: HPOScheduler, objective: callable):\n",
    "        self.save_hyperparameters()\n",
    "        # Bookeeping results for plotting\n",
    "        self.incumbent = None\n",
    "        self.incumbent_error = None\n",
    "        self.incumbent_trajectory = []\n",
    "        self.cumulative_runtime = []\n",
    "        self.current_runtime = 0\n",
    "        self.records = []\n",
    "\n",
    "    def run(self, number_of_trials):\n",
    "        for i in range(number_of_trials):\n",
    "            start_time = time.time()\n",
    "            config = self.scheduler.suggest()\n",
    "            print(f\"Trial {i}: config = {config}\")\n",
    "            error = self.objective(**config)\n",
    "            error = float(error.cpu().detach().numpy())\n",
    "            self.scheduler.update(config, error)\n",
    "            runtime = time.time() - start_time\n",
    "            self.bookkeeping(config, error, runtime)\n",
    "            print(f\"    error = {error}, runtime = {runtime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@d2l.add_to_class(HPOTuner)  #@save\n",
    "def bookkeeping(self, config: dict, error: float, runtime: float):\n",
    "    self.records.append({\"config\": config, \"error\": error, \"runtime\": runtime})\n",
    "    # Check if the last hyperparameter configuration performs better\n",
    "    # than the incumbent\n",
    "    if self.incumbent is None or self.incumbent_error > error:\n",
    "        self.incumbent = config\n",
    "        self.incumbent_error = error\n",
    "    # Add current best observed performance to the optimization trajectory\n",
    "    self.incumbent_trajectory.append(self.incumbent_error)\n",
    "    # Update runtime\n",
    "    self.current_runtime += runtime\n",
    "    self.cumulative_runtime.append(self.current_runtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hpo_objective_lenet(learning_rate, batch_size, max_epochs=10):  #@save\n",
    "    model = d2l.LeNet(lr=learning_rate, num_classes=10)\n",
    "    trainer = d2l.HPOTrainer(max_epochs=max_epochs, num_gpus=1)\n",
    "    data = d2l.FashionMNIST(batch_size=batch_size)\n",
    "    model.apply_init([next(iter(data.get_dataloader(True)))[0]], d2l.init_cnn)\n",
    "    trainer.fit(model=model, data=data)\n",
    "    validation_error = trainer.validation_error()\n",
    "    return validation_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_space = {\n",
    "    \"learning_rate\": stats.loguniform(1e-2, 1),\n",
    "    \"batch_size\": stats.randint(32, 256),\n",
    "}\n",
    "initial_config = {\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"batch_size\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = RandomSearcher(config_space, initial_config=initial_config)\n",
    "scheduler = BasicScheduler(searcher=searcher)\n",
    "tuner = HPOTuner(scheduler=scheduler, objective=hpo_objective_lenet)\n",
    "tuner.run(number_of_trials=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board = d2l.ProgressBoard(xlabel=\"time\", ylabel=\"error\")\n",
    "for time_stamp, error in zip(\n",
    "    tuner.cumulative_runtime, tuner.incumbent_trajectory\n",
    "):\n",
    "    board.draw(time_stamp, error, \"random search\", every_n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
